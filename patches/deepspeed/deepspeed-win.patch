From 1561d9d17c390bc2ac10123cab4c612e615def9d Mon Sep 17 00:00:00 2001
From: Acid Bubbles <acidbubbles.anon@gmail.com>
Date: Fri, 29 Dec 2023 19:15:10 -0500
Subject: [PATCH] Voxta Patch

---
 VOXTA_PATCH.md                                |  3 +++
 build_win.bat                                 |  4 +++
 csrc/quantization/pt_binding.cpp              |  7 ++---
 .../transformer/inference/csrc/pt_binding.cpp | 10 +++----
 deepspeed/accelerator                         |  2 +-
 deepspeed/env_report.py                       | 27 +++++++++----------
 6 files changed, 30 insertions(+), 23 deletions(-)
 create mode 100644 VOXTA_PATCH.md

diff --git a/VOXTA_PATCH.md b/VOXTA_PATCH.md
new file mode 100644
index 00000000..eb919a7b
--- /dev/null
+++ b/VOXTA_PATCH.md
@@ -0,0 +1,3 @@
+# Voxta Patch for DeepSpeed
+
+Source: <https://github.com/oobabooga/text-generation-webui/issues/4734>
diff --git a/build_win.bat b/build_win.bat
index ec8c8a36..c4e9269e 100644
--- a/build_win.bat
+++ b/build_win.bat
@@ -2,6 +2,10 @@
 
 set DS_BUILD_AIO=0
 set DS_BUILD_SPARSE_ATTN=0
+set DS_BUILD_EVOFORMER_ATTN=0
+set DS_SKIP_CUDA_CHECK=1
+set DS_BUILD_CUTLASS_OPS=0
+set DS_BUILD_RAGGED_DEVICE_OPS=0
 
 echo Administrative permissions required. Detecting permissions...
 
diff --git a/csrc/quantization/pt_binding.cpp b/csrc/quantization/pt_binding.cpp
index a4210897..12777603 100644
--- a/csrc/quantization/pt_binding.cpp
+++ b/csrc/quantization/pt_binding.cpp
@@ -241,11 +241,12 @@ std::vector<at::Tensor> quantized_reduction(at::Tensor& input_vals,
                               .device(at::kCUDA)
                               .requires_grad(false);
 
-    std::vector<long int> sz(input_vals.sizes().begin(), input_vals.sizes().end());
-    sz[sz.size() - 1] = sz.back() / devices_per_node;  // num of GPU per nodes
-    const int elems_per_in_tensor = at::numel(input_vals) / devices_per_node;
+    std::vector<int64_t> sz_vector(input_vals.sizes().begin(), input_vals.sizes().end());
+    sz_vector[sz_vector.size() - 1] = sz_vector.back() / devices_per_node;  // num of GPU per nodes
+    at::IntArrayRef sz(sz_vector);
     auto output = torch::empty(sz, output_options);
 
+    const int elems_per_in_tensor = at::numel(input_vals) / devices_per_node;
     const int elems_per_in_group = elems_per_in_tensor / (in_groups / devices_per_node);
     const int elems_per_out_group = elems_per_in_tensor / out_groups;
 
diff --git a/csrc/transformer/inference/csrc/pt_binding.cpp b/csrc/transformer/inference/csrc/pt_binding.cpp
index b7277d1e..a26eaa40 100644
--- a/csrc/transformer/inference/csrc/pt_binding.cpp
+++ b/csrc/transformer/inference/csrc/pt_binding.cpp
@@ -538,8 +538,8 @@ std::vector<at::Tensor> ds_softmax_context(at::Tensor& query_key_value,
     if (layer_id == num_layers - 1) InferenceContext::Instance().advance_tokens();
     auto prev_key = torch::from_blob(workspace + offset,
                                      {bsz, heads, all_tokens, k},
-                                     {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),
-                                      k * InferenceContext::Instance().GetMaxTokenLength(),
+                                     {static_cast<unsigned>(hidden_dim * InferenceContext::Instance().GetMaxTokenLength()),
+                                      static_cast<unsigned>(k * InferenceContext::Instance().GetMaxTokenLength()),
                                       k,
                                       1},
                                      options);
@@ -547,8 +547,8 @@ std::vector<at::Tensor> ds_softmax_context(at::Tensor& query_key_value,
     auto prev_value =
         torch::from_blob(workspace + offset + value_offset,
                          {bsz, heads, all_tokens, k},
-                         {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),
-                          k * InferenceContext::Instance().GetMaxTokenLength(),
+                         {static_cast<unsigned>(hidden_dim * InferenceContext::Instance().GetMaxTokenLength()),
+                          static_cast<unsigned>(k * InferenceContext::Instance().GetMaxTokenLength()),
                           k,
                           1},
                          options);
@@ -1578,7 +1578,7 @@ std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor& input,
     auto output = at::from_blob(output_ptr, input.sizes(), options);
     auto inp_norm = at::from_blob(inp_norm_ptr, input.sizes(), options);
     auto intermediate_gemm =
-        at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);
+        at::from_blob(intermediate_ptr, {input.size(0), input.size(1), static_cast<int64_t>(mlp_1_out_neurons)}, options);
 
     auto act_func_type = static_cast<ActivationFuncType>(activation_type);
 
diff --git a/deepspeed/accelerator b/deepspeed/accelerator
index b61fffac..14bf5923 120000
--- a/deepspeed/accelerator
+++ b/deepspeed/accelerator
@@ -1 +1 @@
-../accelerator/
\ No newline at end of file
+../accelerator
\ No newline at end of file
diff --git a/deepspeed/env_report.py b/deepspeed/env_report.py
index 91226245..1e125bb5 100644
--- a/deepspeed/env_report.py
+++ b/deepspeed/env_report.py
@@ -8,6 +8,8 @@ import torch
 import deepspeed
 import subprocess
 import argparse
+# AFTER
+import psutil
 from .ops.op_builder.all_ops import ALL_OPS
 from .git_version_info import installed_ops, torch_info
 from deepspeed.accelerator import get_accelerator
@@ -108,22 +110,19 @@ def installed_cann_version():
 
 def get_shm_size():
     try:
-        shm_stats = os.statvfs('/dev/shm')
-    except (OSError, FileNotFoundError, ValueError):
-        return "UNKNOWN", None
-
-    shm_size = shm_stats.f_frsize * shm_stats.f_blocks
-    shm_hbytes = human_readable_size(shm_size)
-    warn = []
-    if shm_size < 512 * 1024**2:
-        warn.append(
-            f" {YELLOW} [WARNING] /dev/shm size might be too small, if running in docker increase to at least --shm-size='1gb' {END}"
-        )
-        if get_accelerator().communication_backend_name() == "nccl":
+        temp_dir = os.getenv('TEMP') or os.getenv('TMP') or os.path.join(os.path.expanduser('~'), 'tmp')
+        shm_stats = psutil.disk_usage(temp_dir)
+        shm_size = shm_stats.total
+        shm_hbytes = human_readable_size(shm_size)
+        warn = []
+        if shm_size < 512 * 1024**2:
             warn.append(
-                f" {YELLOW} [WARNING] see more details about NCCL requirements: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data {END}"
+                f" {YELLOW} [WARNING] Shared memory size might be too small, consider increasing it. {END}"
             )
-    return shm_hbytes, warn
+            # Add additional warnings specific to your use case if needed.
+        return shm_hbytes, warn
+    except Exception as e:
+        return "UNKNOWN", [f"Error getting shared memory size: {e}"]
 
 
 def human_readable_size(size):
-- 
2.43.0.windows.1

